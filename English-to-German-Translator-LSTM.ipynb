{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7644b2",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4d05e0",
   "metadata": {},
   "source": [
    "1. choose a dataset\n",
    "2. download and check the shape of the dataset\n",
    "4. clean the data if needed (hint: regex)\n",
    "5. decide the tokenization strategy (word , character , sub character)\n",
    "6. build the vocab\n",
    "7. build a wrapper around the dataset\n",
    "8. dataloader -> train[0] (input,label)\n",
    "9. Build network architecture\n",
    "10. Training loop\n",
    "11. evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94398a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# For loading datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05864d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample English: I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
      "Sample German: Ich bin wirklich begeistert von dieser Konferenz, und ich danke Ihnen allen für die vielen netten Kommentare zu meiner Rede vorgestern Abend.\n",
      "\n",
      "Dataset sizes - Train: 206112, Test: 8079\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"iwslt2017\", \n",
    "    \"iwslt2017-de-en\", \n",
    "    trust_remote_code=True  \n",
    ")\n",
    "\n",
    "train_data = dataset[\"train\"]  \n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "\n",
    "print(f\"Sample English: {train_data[2]['translation']['en']}\")\n",
    "print(f\"Sample German: {train_data[2]['translation']['de']}\")\n",
    "print(f\"\\nDataset sizes - Train: {len(train_data)}, Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19999ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, lang=\"en\"):\n",
    "    \"\"\"Modified from your IMDB function to handle language-specific rules\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text)  \n",
    "    text = re.sub(r'[^\\w\\sßäöüÄÖÜ]', '', text)  # Keep letters + German chars\n",
    "    \n",
    "    # Language-specific rules\n",
    "    if lang == \"en\":\n",
    "        text = text.lower()  # English lowercase    \n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21306e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['thank', 'you', 'so', 'much', 'chris'], ['Vielen', 'Dank', 'Chris'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_translation_data(dataset):\n",
    "    \"\"\"Process all English-German pairs at once, preserving German capitalization\"\"\"\n",
    "    en_texts = []\n",
    "    de_texts = []\n",
    "    \n",
    "    for example in dataset:\n",
    "        # English (lowercase)\n",
    "        en_tokens = preprocess_text(example[\"translation\"][\"en\"], lang=\"en\")\n",
    "        # German (keep capitalization)\n",
    "        de_tokens = preprocess_text(example[\"translation\"][\"de\"], lang=\"de\")\n",
    "        \n",
    "        en_texts.append(en_tokens)\n",
    "        de_texts.append(de_tokens)\n",
    "    \n",
    "    return {\"en_tokens\": en_texts, \"de_tokens\": de_texts}\n",
    "\n",
    "# Apply to all data (no batching)\n",
    "train_data = preprocess_translation_data(dataset[\"train\"]) \n",
    "test_data = preprocess_translation_data(dataset[\"test\"])\n",
    "\n",
    "train_data[\"en_tokens\"][0], train_data[\"de_tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e766b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_translation_vocabs(en_texts, de_texts, max_words=20000):\n",
    "\n",
    "    special_tokens = [\n",
    "        '<PAD>', '<UNK>', '<SOS>', '<EOS>'\n",
    "    ]\n",
    "    \n",
    "    # English vocab\n",
    "    en_vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "    en_counter = Counter(token for text in en_texts for token in text)\n",
    "    for token, _ in en_counter.most_common(max_words - len(special_tokens)):\n",
    "        if token not in en_vocab:\n",
    "            en_vocab[token] = len(en_vocab)\n",
    "    \n",
    "    # German vocab (maintains capitalization)\n",
    "    de_vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "    de_counter = Counter(token for text in de_texts for token in text)\n",
    "    for token, _ in de_counter.most_common(max_words - len(special_tokens)):\n",
    "        if token not in de_vocab:\n",
    "            de_vocab[token] = len(de_vocab)\n",
    "    \n",
    "    return en_vocab, de_vocab\n",
    "\n",
    "en_vocab, de_vocab = build_translation_vocabs(\n",
    "    train_data[\"en_tokens\"], \n",
    "    train_data[\"de_tokens\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f2aa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 20000\n",
      "German vocab size: 20000\n",
      "\n",
      "Sample English tokens: ['thank', 'you', 'so', 'much', 'chris']\n",
      "Sample German tokens: ['Vielen', 'Dank', 'Chris']\n",
      "\n",
      "Special tokens shared:\n",
      "<PAD>: EN=0, DE=0\n"
     ]
    }
   ],
   "source": [
    "print(\"English vocab size:\", len(en_vocab))\n",
    "print(\"German vocab size:\", len(de_vocab))\n",
    "print(\"\\nSample English tokens:\", train_data[\"en_tokens\"][0][:10])\n",
    "print(\"Sample German tokens:\", train_data[\"de_tokens\"][0][:10])\n",
    "print(\"\\nSpecial tokens shared:\")\n",
    "print(f\"<PAD>: EN={en_vocab['<PAD>']}, DE={de_vocab['<PAD>']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "744eaec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max lengths - English: 41, German: 38\n"
     ]
    }
   ],
   "source": [
    "def analyze_lengths(en_texts, de_texts):\n",
    "    en_lengths = [len(seq) for seq in en_texts]\n",
    "    de_lengths = [len(seq) for seq in de_texts]\n",
    "    \n",
    "    en_p95 = int(np.percentile(en_lengths, 95))\n",
    "    de_p95 = int(np.percentile(de_lengths, 95))\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(en_lengths, bins=50, color='blue', alpha=0.7)\n",
    "    plt.axvline(en_p95, color='r', linestyle='--', label=f'95th: {en_p95}')\n",
    "    plt.title(\"English Lengths\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(de_lengths, bins=50, color='green', alpha=0.7)\n",
    "    plt.axvline(de_p95, color='r', linestyle='--', label=f'95th: {de_p95}')\n",
    "    plt.title(\"German Lengths\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return en_p95, de_p95\n",
    "\n",
    "en_max_len, de_max_len = analyze_lengths(train_data[\"en_tokens\"], train_data[\"de_tokens\"])\n",
    "print(f\"Max lengths - English: {en_max_len}, German: {de_max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba789cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens, vocab, max_len, add_special=False):  \n",
    "    \"\"\"Converts tokens to padded IDs (exactly like your IMDB encode_review but with <SOS>/<EOS>).\"\"\"  \n",
    "    if add_special:  \n",
    "        tokens = [\"<SOS>\"] + tokens + [\"<EOS>\"]  \n",
    "    encoded = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens[:max_len]]  \n",
    "    encoded += [vocab[\"<PAD>\"]] * (max_len - len(encoded))  \n",
    "    return encoded  \n",
    "\n",
    "# Encode all data  \n",
    "train_en = [encode(seq, en_vocab, en_max_len) for seq in train_data[\"en_tokens\"]]  \n",
    "train_de = [encode(seq, de_vocab, de_max_len, add_special=True) for seq in train_data[\"de_tokens\"]]  \n",
    "test_en = [encode(seq, en_vocab, en_max_len) for seq in test_data[\"en_tokens\"]]  \n",
    "test_de = [encode(seq, de_vocab, de_max_len, add_special=True) for seq in test_data[\"de_tokens\"]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5a202e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.tensor(train_en), torch.tensor(train_de))  \n",
    "test_data = TensorDataset(torch.tensor(test_en), torch.tensor(test_de))  \n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)  \n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e266198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source shape: torch.Size([41, 64]), Target shape: torch.Size([38, 64])\n"
     ]
    }
   ],
   "source": [
    "for src, trg in test_loader:\n",
    "    \n",
    "    # Transpose to get [seq_len, batch_size]\n",
    "    src = src.transpose(0, 1)\n",
    "    trg = trg.transpose(0, 1)\n",
    "    print(f\"Source shape: {src.shape}, Target shape: {trg.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45bcc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        # Changed from GRU to LSTM\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "\n",
    "        cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1)\n",
    "        \n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "        \n",
    "        cell = torch.tanh(self.fc(cell))\n",
    "        \n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "785d5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [batch_size, dec_hid_dim]\n",
    "        # encoder_outputs = [src_len, batch_size, enc_hid_dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        # Repeat hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # hidden = [batch_size, src_len, dec_hid_dim]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
    "        \n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy = [batch_size, src_len, dec_hid_dim]\n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention = [batch_size, src_len]\n",
    "        \n",
    "        # Return attention weights\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd698e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        # Changed from GRU to LSTM\n",
    "        self.rnn = nn.LSTM((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden_cell, encoder_outputs):\n",
    "        # input = [batch_size]\n",
    "        # hidden_cell = tuple of ([batch_size, dec_hid_dim], [batch_size, dec_hid_dim])\n",
    "        # encoder_outputs = [src_len, batch_size, enc_hid_dim * 2]\n",
    "        \n",
    "        # Unpack hidden and cell states\n",
    "        hidden, cell = hidden_cell\n",
    "        \n",
    "        # Make input 2D\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch_size, emb_dim]\n",
    "        \n",
    "        # Calculate attention\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        # a = [batch_size, src_len]\n",
    "        \n",
    "        # Add batch dimension\n",
    "        a = a.unsqueeze(1)\n",
    "        # a = [batch_size, 1, src_len]\n",
    "        \n",
    "        # Adjust encoder outputs for attention calculation\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]\n",
    "        \n",
    "        # Calculate weighted context vector\n",
    "        context = torch.bmm(a, encoder_outputs)\n",
    "        # context = [batch_size, 1, enc_hid_dim * 2]\n",
    "        \n",
    "        # Adjust context for concatenation\n",
    "        context = context.permute(1, 0, 2)\n",
    "        # context = [1, batch_size, enc_hid_dim * 2]\n",
    "        \n",
    "        # Concatenate embedded input and context vector\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        # rnn_input = [1, batch_size, (enc_hid_dim * 2) + emb_dim]\n",
    "        \n",
    "        # Prepare hidden and cell states for LSTM input\n",
    "        hidden_tensor = hidden.unsqueeze(0)\n",
    "        cell_tensor = cell.unsqueeze(0)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden_new, cell_new) = self.rnn(rnn_input, (hidden_tensor, cell_tensor))\n",
    "        # output = [1, batch_size, dec_hid_dim]\n",
    "        # hidden_new = [1, batch_size, dec_hid_dim]\n",
    "        # cell_new = [1, batch_size, dec_hid_dim]\n",
    "        \n",
    "        # Remove sequence dimension\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        context = context.squeeze(0)\n",
    "        \n",
    "        # Make final prediction\n",
    "        prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))\n",
    "        # prediction = [batch_size, output_dim]\n",
    "        \n",
    "        # Return prediction and the new hidden/cell states\n",
    "        return prediction, (hidden_new.squeeze(0), cell_new.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f19f03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9350cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [src_len, batch_size]\n",
    "        # trg = [trg_len, batch_size]\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode source sequences\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "        \n",
    "        # First decoder input is the < SOS > token\n",
    "        input = trg[0]\n",
    "        \n",
    "        # Start from second token (index 1)\n",
    "        for t in range(1, trg_len):\n",
    "            # Pass through decoder\n",
    "            output, (hidden, cell) = self.decoder(input, (hidden, cell), encoder_outputs)\n",
    "            \n",
    "            # Store output\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            \n",
    "            # Get the highest predicted token\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            # Use actual token or predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ed61fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeb09e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(model.device), trg.to(model.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src.transpose(0, 1), trg.transpose(0, 1))\n",
    "        \n",
    "        # Calculate loss\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg.transpose(0, 1)[1:].reshape(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f56cd4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(model.device), trg.to(model.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src.transpose(0, 1), trg.transpose(0, 1), 0) # no teacher forcing\n",
    "            \n",
    "            # Calculate loss\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg.transpose(0, 1)[1:].reshape(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85f205ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified BLEU calculation function to handle LSTM states\n",
    "def calculate_bleu(model, iterator, trg_vocab, device):\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    idx_to_word = {idx: word for word, idx in trg_vocab.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in iterator:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            # Transpose to get [seq_len, batch_size]\n",
    "            src = src.transpose(0, 1)\n",
    "            trg = trg.transpose(0, 1)\n",
    "            \n",
    "            # Get encoder outputs and hidden/cell states\n",
    "            encoder_outputs, (hidden, cell) = model.encoder(src)\n",
    "            \n",
    "            # First input is < SOS > token\n",
    "            input = trg[0]\n",
    "            \n",
    "            # List to store predicted tokens\n",
    "            pred_tokens = []\n",
    "            \n",
    "            # Maximum prediction length (to avoid infinite loops)\n",
    "            max_len = 50\n",
    "            \n",
    "            # Decode one token at a time\n",
    "            for t in range(1, max_len):\n",
    "                output, (hidden, cell) = model.decoder(input, (hidden, cell), encoder_outputs)\n",
    "                \n",
    "                pred_token = output.argmax(1)\n",
    "                pred_tokens.append(pred_token)\n",
    "                input = pred_token\n",
    "                \n",
    "            # Convert token indices to words\n",
    "            pred_sentences = []\n",
    "            ref_sentences = []\n",
    "            \n",
    "            # Process each sentence in the batch\n",
    "            for i in range(src.shape[1]):\n",
    "                # Prediction\n",
    "                pred_sent = []\n",
    "                for t in range(len(pred_tokens)):\n",
    "                    token = pred_tokens[t][i].item()\n",
    "                    if token == trg_vocab[\"<EOS>\"]:\n",
    "                        break\n",
    "                    elif token not in [trg_vocab[\"<PAD>\"], trg_vocab[\"<SOS>\"]]:\n",
    "                        pred_sent.append(idx_to_word.get(token, \"<UNK>\"))\n",
    "                \n",
    "                # Reference\n",
    "                ref_sent = []\n",
    "                for t in range(1, trg.shape[0]):\n",
    "                    token = trg[t][i].item()\n",
    "                    if token == trg_vocab[\"<EOS>\"]:\n",
    "                        break\n",
    "                    elif token not in [trg_vocab[\"<PAD>\"], trg_vocab[\"<SOS>\"]]:\n",
    "                        ref_sent.append(idx_to_word.get(token, \"<UNK>\"))\n",
    "                \n",
    "                # Only add valid sentences\n",
    "                if pred_sent and ref_sent:\n",
    "                    predictions.append(pred_sent)\n",
    "                    references.append([ref_sent])\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    if not predictions:\n",
    "        return 0.0\n",
    "    \n",
    "    smoothing = SmoothingFunction().method4\n",
    "    return corpus_bleu(references, predictions, smoothing_function=smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "999aaaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_lstm_model(input_dim, output_dim, en_max_len, de_max_len):\n",
    "    # Create attention, encoder, decoder\n",
    "    attention = Attention(512, 512)\n",
    "    encoder = Encoder(input_dim, 256, 512, 0.5)\n",
    "    decoder = Decoder(output_dim, 256, 512, 512, 0.5, attention)\n",
    "\n",
    "    # Create the Seq2Seq model\n",
    "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "    # Initialize weights\n",
    "    def init_weights(m):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "                \n",
    "    model.apply(init_weights)\n",
    "\n",
    "    # Print model details\n",
    "    print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ab34b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, en_vocab, de_vocab, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Translates a sentence from English to German using the model\n",
    "    \"\"\"\n",
    "    # Preprocess the sentence\n",
    "    tokens = preprocess_text(sentence, lang=\"en\")\n",
    "    de_idx_to_word = {idx: word for word, idx in de_vocab.items()}\n",
    "\n",
    "    # Encode with same parameters as training\n",
    "    encoded = [encode(tokens, en_vocab, en_max_len)]\n",
    "    enc_tensor = torch.tensor(encoded).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Prepare input for encoder (transpose to get [seq_len, batch_size])\n",
    "        src_tensor = enc_tensor.transpose(0, 1)\n",
    "        \n",
    "        # Get encoder outputs and hidden/cell states\n",
    "        encoder_outputs, (hidden, cell) = model.encoder(src_tensor)\n",
    "        \n",
    "        # Start with SOS token\n",
    "        input = torch.tensor([de_vocab[\"<SOS>\"]]).to(device)\n",
    "        \n",
    "        # Store generated tokens\n",
    "        translated_tokens = []\n",
    "        \n",
    "        # Generate tokens one at a time\n",
    "        for _ in range(max_length):\n",
    "            # Get prediction from decoder\n",
    "            output, (hidden, cell) = model.decoder(input, (hidden, cell), encoder_outputs)\n",
    "            \n",
    "            # Get most likely word\n",
    "            pred_token = output.argmax(1)\n",
    "            \n",
    "            # Convert to integer\n",
    "            token_id = pred_token.item()\n",
    "            \n",
    "            # Stop if EOS token\n",
    "            if token_id == de_vocab[\"<EOS>\"]:\n",
    "                break\n",
    "                \n",
    "            # Add to results if not a special token\n",
    "            if token_id not in [de_vocab[\"<PAD>\"], de_vocab[\"<SOS>\"]]:\n",
    "                translated_tokens.append(token_id)\n",
    "                \n",
    "            # Use predicted token as next input\n",
    "            input = pred_token\n",
    "    \n",
    "    # Convert IDs to words\n",
    "    translated_words = [de_idx_to_word.get(idx, \"<UNK>\") for idx in translated_tokens]\n",
    "    \n",
    "    # Format the final sentence\n",
    "    if not translated_words:\n",
    "        return \"\"  # Empty if no valid words\n",
    "    \n",
    "    # Join words and fix German formatting\n",
    "    sentence = \" \".join(translated_words)\n",
    "    sentence = re.sub(r' (?=[\\.,!?])', '', sentence)  # Fix punctuation spacing\n",
    "    return sentence.capitalize()  # Capitalize first letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b6f3617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 54,240,288 trainable parameters\n",
      "Epoch 1:\n",
      "\tTrain Loss: 5.2481\n",
      "\tValid Loss: 4.6774\n",
      "\tBLEU Score: 8.59%\n",
      "Epoch 2:\n",
      "\tTrain Loss: 3.9870\n",
      "\tValid Loss: 4.3589\n",
      "\tBLEU Score: 12.21%\n",
      "Epoch 3:\n",
      "\tTrain Loss: 3.5043\n",
      "\tValid Loss: 4.3246\n",
      "\tBLEU Score: 14.04%\n",
      "Epoch 4:\n",
      "\tTrain Loss: 3.2212\n",
      "\tValid Loss: 4.2925\n",
      "\tBLEU Score: 14.48%\n",
      "Epoch 5:\n",
      "\tTrain Loss: 3.0151\n",
      "\tValid Loss: 4.3488\n",
      "\tBLEU Score: 14.53%\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(en_vocab)\n",
    "output_dim = len(de_vocab)\n",
    "model = initialize_lstm_model(input_dim, output_dim, en_max_len, de_max_len)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=de_vocab[\"<PAD>\"])\n",
    "\n",
    "# Training parameters\n",
    "epochs = 5\n",
    "clip = 1.0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip)\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f}')\n",
    "\n",
    "    valid_loss = evaluate(model, test_loader, criterion)\n",
    "    print(f'\\tValid Loss: {valid_loss:.4f}')\n",
    "\n",
    "    bleu = calculate_bleu(model, test_loader, de_vocab, device)\n",
    "    print(f'\\tBLEU Score: {bleu*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6c19aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"translation_lstm_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f1155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: I am so proud of what i have done.\n",
      "German: Ich bin so stolz darauf was ich getan habe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test translation\n",
    "english_sentence = \"I am so proud of what I have done.\"\n",
    "translation = translate_sentence(english_sentence, model, en_vocab, de_vocab, device)\n",
    "print(f\"English: {english_sentence}\")\n",
    "print(f\"German: {translation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
